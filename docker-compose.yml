services:
  sd:
    build: .
    image: stable-diffusion:latest
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HF_TOKEN=${HF_TOKEN}
    volumes:
      # Output images to host
      - ./output:/app/output
      # Cache models on host to avoid re-downloading
      - sd-models:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    # Override for interactive use
    stdin_open: true
    tty: true

  # SDXL base + refiner (best quality, needs ~14GB VRAM)
  sdxl:
    build: .
    image: stable-diffusion:latest
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HF_TOKEN=${HF_TOKEN}
    volumes:
      - ./output:/app/output
      - sd-models:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    entrypoint: ["python3", "generate_sdxl.py"]
    stdin_open: true
    tty: true

volumes:
  sd-models:
    name: stable-diffusion-models
